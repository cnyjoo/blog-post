---
type: docs
bookToc: True
weight: 1
---

# MIXTURE OF LORA EXPERTS

## Background

### What is LoRA?
LoRA is a methodology for effective fine-tuning large-scale pretrained models. Models such as OPT, LLaMA, and CLIP demonstrate remarkable performance when fine-tuned for various downstream tasks. However, full fine-tuning of these massive models requires substantial computational resources. LoRA enables parameter-efficient fine-tuning by keeping the pretrained model's weights frozen and adding trainable low-rank decomposition matrices.

### LoRAs Composistion

### Mixture-of-Experts

## Mixture of LoRA experts

### Motivations

### Method

### Training

## Results

## Analyisis and Limitations
